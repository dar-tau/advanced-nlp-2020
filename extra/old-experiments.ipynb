{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negativeModel = AutoModel.from_pretrained('bert-base-uncased', output_attentions = True, cache_dir = cache_dir)\n",
    "positiveModel = AutoModel.from_pretrained('bert-base-uncased', output_attentions = True, cache_dir = cache_dir)\n",
    "\n",
    "data = {}\n",
    "tokenizedData = {}\n",
    "for i, modelName in tqdm(enumerate(['negativeModel', 'positiveModel']), total = 2):\n",
    "    data[modelName] = [sent for sent, targ in zip(sentences, target) if targ == i]\n",
    "    tokenizedData[modelName] = bertTokenizerFast.batch_encode_plus(data[modelName], pad_to_max_length = True, \n",
    "                                                               max_length=512, return_tensors='pt', truncation=True)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetFromTokenized(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i].ids)\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "negativeModel = BertForMaskedLM.from_pretrained('bert-base-uncased', cache_dir = cache_dir)\n",
    "positiveModel = BertForMaskedLM.from_pretrained('bert-base-uncased', cache_dir = cache_dir)\n",
    "\n",
    "negativeModel.train()\n",
    "positiveModel.train()\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "optimizer_grouped_parameters = {}\n",
    "optimizer = {}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "trainer = {}\n",
    "\n",
    "for modelName, model in (('positiveModel', positiveModel), ('negativeModel', negativeModel)):\n",
    "    \n",
    "    optimizer_grouped_parameters[modelName] =  [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer[modelName] = AdamW(optimizer_grouped_parameters[modelName], lr=1e-5)\n",
    "    \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer = bertTokenizerFast, mlm=True, mlm_probability=0.15\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/vol/scratch/guy/\" + modelName,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=20,\n",
    "        per_gpu_train_batch_size=4\n",
    "    )\n",
    "\n",
    "    trainer[modelName] = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        data_collator = data_collator,\n",
    "        train_dataset = DatasetFromTokenized(tokenizedData[modelName]),\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "for _, t in trainer.items():\n",
    "    t.train()\n",
    "\n",
    "sequence = \"she is very [MASK].\"\n",
    "input = bertTokenizerFast.encode(sequence, return_tensors=\"pt\").cuda()\n",
    "mask_token_index = torch.where(input == bertTokenizerFast.mask_token_id)[1]\n",
    "\n",
    "token_logits = negativeModel(input)[0]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(bertTokenizerFast.mask_token, bertTokenizerFast.decode([token])))\n",
    "\n",
    "input = bertTokenizerFast.encode(sequence, return_tensors=\"pt\").cuda()\n",
    "mask_token_index = torch.where(input == bertTokenizerFast.mask_token_id)[1]\n",
    "\n",
    "token_logits = positiveModel(input)[0]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(bertTokenizerFast.mask_token, bertTokenizerFast.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissecting BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Dissect BERT\n",
    "\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "mlmModel = BertForMaskedLM.from_pretrained('bert-base-uncased', output_attentions = False, cache_dir = cache_dir)\n",
    "\n",
    "### Try to make BERT angry\n",
    "\n",
    "embeddings = dict(mlmModel.base_model.named_children())['embeddings']\n",
    "\n",
    "_bad_token = 2919\n",
    "_bad_emb = embeddings(torch.LongTensor([[_bad_token]])).squeeze()\n",
    "\n",
    "def _add_bad_emb(module, inp, outp):\n",
    "    return f(module, inp ,outp)\n",
    "hookHandle = embeddings.register_forward_hook(_add_bad_emb)\n",
    "\n",
    "def f1(module, inp, outp):\n",
    "    old_norm = torch.norm(outp)\n",
    "    new_outp = outp +  _bad_emb\n",
    "    new_norm = torch.norm(new_outp)\n",
    "    norm_ratio = new_norm / old_norm\n",
    "    return new_outp / norm_ratio\n",
    "\n",
    "\n",
    "def f2(module, inp, outp):\n",
    "    noise = (_bad_emb - outp)\n",
    "    rate = 1\n",
    "    new_outp = (1-rate) * outp +  rate * noise\n",
    "    return new_outp\n",
    "\n",
    "def f(module, inp, outp):\n",
    "    pass\n",
    "\n",
    "old_mask_emb = embeddings.word_embeddings.weight[bertTokenizer.mask_token_id]\n",
    "\n",
    "bertTokenizer(\"she\")\n",
    "_she_token = 2016\n",
    "\n",
    "embeddings.word_embeddings.weight[:] +=   .5* (embeddings.word_embeddings\n",
    "                                                                        .weight[_bad_token])\n",
    "\n",
    "### Play with Layers\n",
    "\n",
    "#### Old\n",
    "\n",
    "bertLayers = list(mlmModel.base_model.encoder.layer.children())\n",
    "\n",
    "hookHandle = {}\n",
    "\n",
    "def fLayer(module, inp, outp):\n",
    "    if module.isFirst:\n",
    "        module.isFirst = False\n",
    "#        return (module(module(outp[0])[0])[0],)\n",
    "\n",
    "layerNum = 4 \n",
    "\n",
    "hookHandle[layerNum].remove()\n",
    "\n",
    "bertLayers[layerNum].isFirst = True\n",
    "hookHandle[layerNum] = bertLayers[layerNum].register_forward_hook(fLayer)\n",
    "\n",
    "bertLayers = list(mlmModel.base_model.encoder.layer.children())\n",
    "\n",
    "hookHandle = {}\n",
    "\n",
    "def fLayer(module, inp, outp):\n",
    "    if module.isFirst:\n",
    "        module.isFirst = False\n",
    "#        return (module(module(outp[0])[0])[0],)\n",
    "\n",
    "layerNum = 4 \n",
    "\n",
    "hookHandle[layerNum].remove()\n",
    "\n",
    "bertLayers[layerNum].isFirst = True\n",
    "hookHandle[layerNum] = bertLayers[layerNum].register_forward_hook(fLayer)\n",
    "\n",
    "#### New\n",
    "\n",
    "myLayers = mlmModel.base_model.encoder.layer\n",
    "\n",
    "myLayers.insert(0, myLayers[4])\n",
    "\n",
    "#### Fill Mask\n",
    "\n",
    "sequence = \"She is like that sometimes. Don't let it bother you. She is [MASK].\"\n",
    "input = bertTokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input == bertTokenizer.mask_token_id)[1]\n",
    "\n",
    "token_logits = mlmModel(input)[0]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(bertTokenizer.mask_token, bertTokenizer.decode([token])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guy-python",
   "language": "python",
   "name": "guy-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
